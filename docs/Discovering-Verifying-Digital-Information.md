# Gathering and Verifying Digital Information
Defend yourself against fake news and extraordinary claims!

### Introduction

In this course, our goal is to build an understanding of how information is organized on the internet to improve your ability to discover truth and grow your knowledge. We'll start with some basics about what information is at a theoretical level before diving into some practical methods for finding high-quality information on the internet.

Here's the outline we'll cover:

- Intro to Information Theory
- Composition of Informative Data
- Information Sources, Trust, & Reputation
- Incentive Models for Information Sharing
- Information Structures on the Internet
- Applied Internet Research Tools

### Intro to Information Theory

As a field of study, information theory can get very abstract and there's a rabbit hole into mathematics & modern physics that I'd like to avoid for the purposes of this course. To make sure we can stay on track and relate these ideas to practical applications, lets ask ourselves a few questions first:

- Where does information come from?
- What does information look like?
- Why do we need information and how does it relate to our sense of truth?

The main difficulty in understanding the concepts in this course stems from the fact that we are performing a "meta-analysis" about knowledge itself. It's obvious that someone can learn to become an expert in physics or history by digesting _information_ about physics or history. However, we are trying to become an expert in information so digesting information about information seems self-referential and this gives our brains a harder time trying to make sense of some of these ideas.

Information itself has a composition and nature and there are strong scientific principles that dictate how we transmit, receive, and process information. Let's try and relate to these concepts by thinking about how we receive information first.

Usually we sense the information that we read online by using our eyesight.

The pixels on this computer are small enough to be organized into text which serve as the *input* that your eyes use as a signal. The first thing this signal tells us is that there may be some kind of information on the screen that could be valuable to use. Then that signal gets processed through your brain to match it up against your knowledge of the English language and before you know it you understand what the text means. The conclusions about what you are reading materialize into your consciousness and let's say you then write them into your notebook. These notes represent an *output* signal.

We've now constructed an information system in our imagination that takes an input and processes it into an output. Once we've defined this system we can start to ask questions about how reliably the input is being received (is the text you're reading legible?), what exactly is happening during the processing step (are you fully understanding the meaning of each word you read? Or are there 1 or 2 words you aren't familiar with?), and what exactly does the output look like (do the notes you took resemble the text you read?).

At this point we're still at a high level of abstraction and we want to work our way down to specific, practical examples, but just know we still have flexibility in constructing these input/output systems and there are no right or wrong answers just yet. The important takeaway is that identifying inputs, outputs, and processes will help use figure out how to extract high-quality information.

### Information Sources, Trust, & Reputation

The source of a piece of information is highly relevant in the world of news and journalism so let's pull some of this theory into evaluating how well sources can provide information about what's going on in the world.

The first important idea here is that there is a person, or at least the influence of a person, behind *everything*. Bots and algorithms were programmed and devised by people and there's usually someone who understands the exact mechanics of something that seems automagical. News articles are written by people who you are presumably trusting to do the hard research before making any claims of fact.

Understanding the motivations of people behind information sources can also help your evaluation. Does this person have anything at stake for spreading bad information? Will they be fired or subjected to public scrutiny? Have they personally invested in their own news organization or journalistic brand? Are there any likely and conceivable reasons that this person should tell the truth?

It's helpful to keep track of the credibility of your sources for your own convenience, but remain vigilant because everyone makes mistakes. Reputation matters, and you should hold to account the people that you consider "experts" when they get something wrong.

### Incentive Models for Information Sharing

A source may or may not have a specific reason to share certain information. Consider a news publication that runs paid ads on their website. The business economics are simple: the more views & clicks they can get to their website, the more money they make, and the more they can pay to hire better writers and continue to grow their business.

What tends to happen over time is that the publication will prioritize the number of clicks an article is likely to get over the actual quality of information being presented in the article. This doesn't mean that every article becomes fake news, but the effect may be much more subtle. Writers feel a pressure (however slight it may be) to make sure their articles attract enough clicks because after a long enough time there won't be enough money to pay the writers.

What we've described is called an *incentive model* and this one in particular doesn't do a very good job of ensuring the spread of high-quality information. If a news publication is instead set up to accept monthly subscriptions from its readers instead of running paid ads, then the incentive shifts from attract the most clicks to attrating paying readers. The idea here is that readers who are willing to pay will only do so because they trust a publication to put out the highest quality information. This second model is one where reputation matters and the publication has plenty to lose if their readers start canceling their subscriptions due to bad information.

#### Resource Cost Evaluation

Most of the time it's just not realistic for the average person to verify all of the evidence behind every single claim to make sure it's legitimate. You read a news article and trust the publisher to have an incentive to get the facts right, but even if they provide reference links to scientific papers and none of their sources are anonymous, you still have to spend a lot of time clicking around and reading their supporting evidence.

Alternatively, it may be easier to try to determine the logistics required to actually obtain the evidence that is being presented. For example, a journalist reports that a new species of whale has been discovered near the bottom of the ocean. This is obviously difficult to verify because you can't possibly go check the bottom of the ocean to make sure, but there are some questions you could challenge yourself to answer:

- What evidence exists to support this claim?
- Has the journalist obtained video footage, sound recordings or infrared readings?
- If so, would you be able to conclude that the evidence is legitimate just by observing these readings? Or do you lack an understanding of sonar or infrared signals to identify possible signs of falsified evidence or miscalculations?
- Is it reasonable to assume that the journalist was even capable of purchasing and operating the equipment required to measure these readings? Or did they have help from some kind of specialist?

Some of these questions will lead to further questions, and this is perfectly okay as long as you keep track of the original claim. Remember, we are trying to confirm or deny the existence of a new species of whale. Let's say you find out that it's more likely that the journalist obtained sonar readings from a marine biologist at a research facility that definitely has access to such equipment. You must now decide whether to trust this next point in the source chain or investigate further as to whether or not the marine biologist could have miscalculated or falsified the evidence.

Though you will never get perfect answers to these questions, you will at least be able to construct the possible pathways through which information travels. You can then identify which pathways would be more susceptible to error and assign a sort of "probability of truth" to each pathway. If you find evidence that the journalist is a known liar, you can lower the probability of that pathway without even having to assess the professional standards of a marine biologist. If you find evidence that the marine biologist was using sonar equipment that had many reports of defective readings, then you can lower the probability on that pathway as well.

There is no objective standard for how much evidence is "enough" and this is an important takeaway. You might think that these things need to be binary and a claim is either true or false, fact or opinion... but there's actually a lot of complexity and nuance to discovering the truth and verifying information. There are a lot of blurry lines drawn around malicious lying, irresponsible negligence, accidental ignorance, acceptable standards, and omitted margins of error. When someone makes a claim of fact, you never really know their intentions and where the verification of their claims might take you.


## Course Outline
### 1. Intro to Information Theory


### 2. Applied Research Tools and Practices

  - Offering this early on for the quick and dirty. Dive deeper into digital verification after this chapter.
  - Optimizing Google Searches with specific time frames

#### Optimizing Google Searches

  To use Google's search engine as a resource for fathering information, you enter a string of words and numbers into the search box. The results that appear are determined by an algorithm that has been continually maintained, developed, and secured for years. Generally the algorithm makes an assessment on the link for each result and bases it on things like:
  -- Number of visits to the website
  -- How long it's been since anyone had first visited the website
  -- How many other websites that already exist on the Internet link to and from the result


  Using Google search results to determine what was understood

  As human developers continue to work on the algorithm, it advances and grows in complexity. Filtering the first page of results to within a time-frame builds context (provide examples)

  - How to navigate Wikipedia and understand its composition


### Using the Scientific Cycle to verify information served by links

- Innovations AND new problems, both simultaneously considered
- Understanding that there is a person behind everything, including bots and algorithms

